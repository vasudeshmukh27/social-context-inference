{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the library for Google Cloud Storage\n!pip install --upgrade gcsfs google-cloud-storage pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:22:49.846962Z","iopub.execute_input":"2025-08-19T05:22:49.847272Z","iopub.status.idle":"2025-08-19T05:23:23.186090Z","shell.execute_reply.started":"2025-08-19T05:22:49.847241Z","shell.execute_reply":"2025-08-19T05:23:23.184988Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.2)\nCollecting gcsfs\n  Downloading gcsfs-2025.7.0-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\nCollecting google-cloud-storage\n  Downloading google_cloud_storage-3.3.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting pandas\n  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.12.13)\nRequirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\nCollecting fsspec==2025.7.0 (from gcsfs)\n  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.40.3)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.4)\nCollecting google-api-core<3.0.0,>=2.15.0 (from google-cloud-storage)\n  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\nRequirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.20.1)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.70.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (3.20.3)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.26.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.6.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading gcsfs-2025.7.0-py2.py3-none-any.whl (36 kB)\nDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_cloud_storage-3.3.0-py3-none-any.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.3/274.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, google-api-core, google-cloud-storage, gcsfs, pandas\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 1.34.1\n    Uninstalling google-api-core-1.34.1:\n      Successfully uninstalled google-api-core-1.34.1\n  Attempting uninstall: google-cloud-storage\n    Found existing installation: google-cloud-storage 2.19.0\n    Uninstalling google-cloud-storage-2.19.0:\n      Successfully uninstalled google-cloud-storage-2.19.0\n  Attempting uninstall: gcsfs\n    Found existing installation: gcsfs 2025.3.2\n    Uninstalling gcsfs-2025.3.2:\n      Successfully uninstalled gcsfs-2025.3.2\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.25.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ngoogle-cloud-aiplatform 1.99.0 requires google-cloud-storage<3.0.0,>=1.32.0, but you have google-cloud-storage 3.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.7.0 gcsfs-2025.7.0 google-api-core-2.25.1 google-cloud-storage-3.3.0 pandas-2.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import the necessary modules\nimport json\nfrom kaggle_secrets import UserSecretsClient\nimport gcsfs\n\n# Authenticate with Google Cloud using the Secret\n# Retrieve the secret you stored under the label 'GCS_CREDENTIALS'\nuser_secrets = UserSecretsClient()\ngcp_credentials = user_secrets.get_secret(\"GCS_CREDENTIALS\")\ngcp_creds_dict = json.loads(gcp_credentials)\n\n# Create a GCS filesystem object, passing the credentials directly\nfs = gcsfs.GCSFileSystem(token=gcp_creds_dict)\n\nprint(\"✅ Authentication successful!\")\nprint(\"GCS filesystem is ready to use.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:24:18.987884Z","iopub.execute_input":"2025-08-19T05:24:18.988208Z","iopub.status.idle":"2025-08-19T05:24:20.208233Z","shell.execute_reply.started":"2025-08-19T05:24:18.988177Z","shell.execute_reply":"2025-08-19T05:24:20.207106Z"}},"outputs":[{"name":"stdout","text":"✅ Authentication successful!\nGCS filesystem is ready to use.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Define your bucket name\nBUCKET_NAME = \"ring-ami-dataset-storage\"\n\n# List the contents of your bucket to confirm the connection\nprint(f\"Contents of gs://{BUCKET_NAME}/:\")\nfile_list = fs.ls(f\"gs://{BUCKET_NAME}\")\nfor file_path in file_list:\n    print(file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:27:46.634221Z","iopub.execute_input":"2025-08-19T05:27:46.634618Z","iopub.status.idle":"2025-08-19T05:27:48.538281Z","shell.execute_reply.started":"2025-08-19T05:27:46.634495Z","shell.execute_reply":"2025-08-19T05:27:48.537337Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Contents of gs://ring-ami-dataset-storage/:\nring-ami-dataset-storage/MiniLibriMix\nring-ami-dataset-storage/ami_public_manual_1.6.2\nring-ami-dataset-storage/array1-01\nring-ami-dataset-storage/array1-02\nring-ami-dataset-storage/array1-03\nring-ami-dataset-storage/array1-04\nring-ami-dataset-storage/array1-05\nring-ami-dataset-storage/array1-06\nring-ami-dataset-storage/array1-07\nring-ami-dataset-storage/array1-08\nring-ami-dataset-storage/headset\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Stage 1 — Build a canonical speaker map from AMI meetings.xml\n\n# Standard library XML parser for reading the AMI manifest\nimport xml.etree.ElementTree as ET\n\n# Notebook-friendly progress utility (import preserved for parity, even if unused)\nfrom tqdm.notebook import tqdm\n\n\n# Path to the AMI master manifest that enumerates meetings and speakers\nMEETINGS_XML_PATH = \"ring-ami-dataset-storage/ami_public_manual_1.6.2/corpusResources/meetings.xml\"\n\n# Trace log: announce which manifest is being parsed\nprint(f\"Attempting to parse master file: {MEETINGS_XML_PATH}\")\n\n# Accumulator for results: {meeting_id: {channel:int -> speaker:str}}\nchannel_to_speaker_map = {}\n\n\n# Guard the I/O and XML parsing so we can surface a clear, single error message\ntry:\n\n    # Open the AMI manifest via the provided filesystem handle; AMI text is ISO-8859-1 encoded\n    with fs.open(MEETINGS_XML_PATH, 'r', encoding='ISO-8859-1') as f:\n\n        # Parse the XML stream into an ElementTree\n        tree = ET.parse(f)\n        \n        # Get the root node (<meetings>)\n        root = tree.getroot()\n        \n        # Iterate over each <meeting> element in the manifest\n        for meeting in root.findall('meeting'):\n\n            # Extract the meeting identifier from the 'observation' attribute\n            meeting_id = meeting.get('observation')\n\n            # Skip any malformed entries that do not expose an ID\n            if meeting_id:\n\n                # Initialize the per-meeting channel→speaker map\n                channel_to_speaker_map[meeting_id] = {}\n                \n                # Enumerate all <speaker> elements within this meeting\n                for speaker in meeting.findall('speaker'):\n\n                    # Channel index as declared in the manifest (string)\n                    channel = speaker.get('channel')\n                    \n                    # The speaker's name is in the 'global_name' attribute\n                    speaker_name = speaker.get('global_name')\n\n                    # Only materialize mappings when both channel and name are present\n                    if channel and speaker_name:\n                        \n                        # Store the mapping: {meeting_id: {channel: speaker_name}}\n                        channel_to_speaker_map[meeting_id][int(channel)] = speaker_name\n\n    # Success path: confirm build and print basic sanity checks\n    print(\"\\n Master speaker map built successfully!\")\n    print(f\"Found mappings for {len(channel_to_speaker_map)} meetings.\")\n    print(\"\\n--- Example mapping for meeting 'IS1000a': ---\")\n    print(channel_to_speaker_map.get('IS1000a'))\n\n# Consolidated failure path: emit the exception for actionable debugging\nexcept Exception as e:\n    print(f\"\\n An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:31:19.308429Z","iopub.execute_input":"2025-08-19T05:31:19.308761Z","iopub.status.idle":"2025-08-19T05:31:23.692745Z","shell.execute_reply.started":"2025-08-19T05:31:19.308738Z","shell.execute_reply":"2025-08-19T05:31:23.692019Z"}},"outputs":[{"name":"stdout","text":"Attempting to parse master file: ring-ami-dataset-storage/ami_public_manual_1.6.2/corpusResources/meetings.xml\n\n✅ Master speaker map built successfully!\nFound mappings for 171 meetings.\n\n--- Example mapping for meeting 'IS1000a': ---\n{3: 'MIO016', 1: 'MIO082', 0: 'FIE081', 2: 'MIO050'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# STAGE 2: Use the map to parse the segment files correctly\n\n# (This assumes the 'channel_to_speaker_map' from Stage 1 is in memory)\n\n# Standard library XML parser for segment-level annotations\nimport xml.etree.ElementTree as ET\n\n# Notebook-friendly progress display for batch file iteration\nfrom tqdm.notebook import tqdm\n\n# DataFrame utilities for structuring and aggregating parsed annotations\nimport pandas as pd\n\n\n# Path to the AMI directory that holds per-meeting segment XMLs\nSEGMENTS_PATH = \"ring-ami-dataset-storage/ami_public_manual_1.6.2/segments\"\n\n# Enumerate all segment XML files available in the path\nsegment_files = fs.glob(f\"{SEGMENTS_PATH}/*.xml\")\n\n# Accumulator for parsed results across all meetings\nall_speaker_segments = []\n\n# Iterate through every segment file with a progress bar\nfor file_path in tqdm(segment_files, desc=\"Parsing Segments with Map\"):\n    \n    # Derive the meeting identifier directly from the filename\n    meeting_id = file_path.split('/')[-1].split('.')[0]\n    \n    # Skip files for which no channel→speaker mapping is available\n    if meeting_id not in channel_to_speaker_map:\n        continue\n        \n    try:\n        # Open and parse the XML segment file using the known encoding\n        with fs.open(file_path, 'r', encoding='ISO-8859-1') as f:\n            tree = ET.parse(f)\n            root = tree.getroot()\n\n            # Iterate through each <segment> element for this meeting\n            for segment in root.findall('segment'):\n\n                # Ensure segment exposes both channel and time boundaries\n                if 'channel' in segment.attrib and 'transcriber_start' in segment.attrib and 'transcriber_end' in segment.attrib:\n\n                    # Channel is the index used to resolve the speaker ID\n                    channel = int(segment.attrib['channel'])\n                    \n                    # Use the map to find the speaker ID\n                    speaker_id = channel_to_speaker_map[meeting_id].get(channel)\n\n                    # Only materialize the record if a valid mapping exists\n                    if speaker_id: # Only add if we found a speaker\n                        all_speaker_segments.append({\n                            \"meeting_id\": meeting_id,\n                            \"speaker_id\": speaker_id,\n                            \"begin_time\": float(segment.attrib['transcriber_start']),\n                            \"end_time\": float(segment.attrib['transcriber_end'])\n                        })\n\n    # If XML is malformed or unreadable, emit a warning and skip gracefully\n    except ET.ParseError:\n        print(f\"\\nWarning: Could not parse XML file: {file_path}. Skipping.\")\n        continue\n\n\n# --- Validation and aggregation ---\nif not all_speaker_segments:\n\n    # No usable annotations were found across the dataset\n    print(\"\\n❌ ERROR: No valid segment data found even with the speaker map.\")\n    \nelse:\n    # Construct a DataFrame with all parsed speaker segments\n    annotations_df = pd.DataFrame(all_speaker_segments)\n\n    # Group per meeting and consolidate timelines into dicts for convenience\n    annotations_by_meeting = annotations_df.groupby('meeting_id').apply(\n        lambda x: x[['speaker_id', 'begin_time', 'end_time']].to_dict('records')\n    ).to_dict()\n\n    print(\"\\n Speaker timelines parsed successfully using the map!\")\n    print(f\"Processed data for {len(annotations_by_meeting)} unique meetings.\")\n    \n    # Display a sample of the output for a single meeting\n    print(\"\\n--- First 5 speaker segments for a sample meeting: ---\")\n    sample_meeting_key = next(iter(annotations_by_meeting))\n    print(f\"Sample Meeting ID: {sample_meeting_key}\")\n    print(annotations_by_meeting.get(sample_meeting_key)[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:35:33.510629Z","iopub.execute_input":"2025-08-19T05:35:33.510941Z","iopub.status.idle":"2025-08-19T05:46:00.882389Z","shell.execute_reply.started":"2025-08-19T05:35:33.510920Z","shell.execute_reply":"2025-08-19T05:46:00.881315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Parsing Segments with Map:   0%|          | 0/687 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deeafd3d2df74bf984569f5a484683d1"}},"metadata":{}},{"name":"stdout","text":"\n✅ Speaker timelines parsed successfully using the map!\nProcessed data for 171 unique meetings.\n\n--- First 5 speaker segments for a sample meeting: ---\nSample Meeting ID: EN2001a\n[{'speaker_id': 'MEE068', 'begin_time': 5.496, 'end_time': 6.07}, {'speaker_id': 'MEE068', 'begin_time': 11.04, 'end_time': 15.632}, {'speaker_id': 'MEE068', 'begin_time': 18.883, 'end_time': 19.373}, {'speaker_id': 'MEE068', 'begin_time': 21.392, 'end_time': 25.857}, {'speaker_id': 'MEE068', 'begin_time': 28.033, 'end_time': 40.152}]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3167552060.py:47: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  annotations_by_meeting = annotations_df.groupby('meeting_id').apply(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# FINAL STAGE: Sliding Window Processing with Accurate File Paths\n\nimport numpy as np\nimport soundfile as sf\nfrom tqdm.notebook import tqdm\nimport pickle\n\n# --- Runtime Configuration ---\n# Size of each sliding analysis window (in seconds)\nWINDOW_SIZE = 3.0\n\n# Hop interval between consecutive windows (in seconds)\nHOP_SIZE = 1.5\n\n# Hard limit on number of meetings to process (None = process all available)\nMEETING_LIMIT = 5\n\n# Audio sampling frequency in Hz\nSAMPLE_RATE = 16000\n\n# Define GCS paths\nHEADSET_AUDIO_PATH = \"ring-ami-dataset-storage/headset\"\nARRAY_AUDIO_PATH = \"ring-ami-dataset-storage/array1-01\"\n\n# Output pickle file to store processed dataset\nOUTPUT_PATH = \"/kaggle/working/processed_ami_data.pkl\"\n\nprint(\"Starting final processing with corrected file paths...\")\n\n# Limit meetings if MEETING_LIMIT is set\nmeeting_ids_to_process = list(annotations_by_meeting.keys())\nif MEETING_LIMIT:\n    meeting_ids_to_process = meeting_ids_to_process[:MEETING_LIMIT]\n\n# Master accumulator for all sliding window segments across meetings\nprocessed_data = []\n\n# Iterate through every selected meeting with progress feedback\nfor meeting_id in tqdm(meeting_ids_to_process, desc=\"Processing Meetings\"):\n\n    # Retrieve segment-level annotations for this meeting\n    annotations = annotations_by_meeting.get(meeting_id, [])\n    if not annotations:\n        continue\n\n    try:\n        # --- HEADSET AUDIO LOADING ---\n        # 1. Load and mix all headset channels for a clean mix\n        headset_files = fs.glob(f\"{HEADSET_AUDIO_PATH}/{meeting_id}/audio/{meeting_id}.Headset-*.wav\")\n        if not headset_files:\n            print(f\"Warning: No headset audio files found for {meeting_id}. Skipping.\")\n            continue\n\n        headset_tracks = []\n        for file_path in headset_files:\n            with fs.open(file_path, 'rb') as f:\n                audio, _ = sf.read(f)\n                headset_tracks.append(audio)\n        \n        # Mix tracks by summing them. Find the length of the longest track for padding.\n        max_len = max(len(track) for track in headset_tracks)\n        ihm_full_audio = np.zeros(max_len)\n        for track in headset_tracks:\n            ihm_full_audio[:len(track)] += track\n\n        # 2. Load the single array microphone track\n        array_file = fs.glob(f\"{ARRAY_AUDIO_PATH}/{meeting_id}/audio/{meeting_id}.Array1-01.wav\")[0]\n        with fs.open(array_file, 'rb') as f:\n            sdm_full_audio, _ = sf.read(f)\n\n    except (IndexError, FileNotFoundError):\n        print(f\"Warning: Could not find all required audio files for meeting {meeting_id}. Skipping.\")\n        continue\n\n    meeting_duration = max(ann['end_time'] for ann in annotations)\n\n    # --- Sliding Window ---\n    for window_start in np.arange(0, meeting_duration - WINDOW_SIZE, HOP_SIZE):\n        window_end = window_start + WINDOW_SIZE\n        \n        active_speakers = {ann['speaker_id'] for ann in annotations if ann['begin_time'] < window_end and ann['end_time'] > window_start}\n        num_speakers = len(active_speakers)\n\n        label = \"Group Discussion\"\n        if num_speakers == 0:\n            label = \"Alone / Quiet\"\n        elif num_speakers == 1:\n            label = \"Speech / Monologue\"\n        elif num_speakers == 2:\n            label = \"One-on-One Conversation\"\n\n        start_sample = int(window_start * SAMPLE_RATE)\n        end_sample = int(window_end * SAMPLE_RATE)\n        \n        ihm_clip = ihm_full_audio[start_sample:end_sample]\n        sdm_clip = sdm_full_audio[start_sample:end_sample]\n        \n        expected_len = int(WINDOW_SIZE * SAMPLE_RATE)\n        if len(ihm_clip) < expected_len:\n            ihm_clip = np.pad(ihm_clip, (0, expected_len - len(ihm_clip)))\n        if len(sdm_clip) < expected_len:\n            sdm_clip = np.pad(sdm_clip, (0, expected_len - len(sdm_clip)))\n            \n        processed_data.append({\n            \"meeting_id\": meeting_id, \"label\": label,\n            \"ihm_audio\": ihm_clip, \"sdm_audio\": sdm_clip,\n            \"sample_rate\": SAMPLE_RATE,\n        })\n\n# --- Save Final Data ---\nprint(f\"\\n Successfully generated {len(processed_data)} labeled audio clips.\")\nprint(f\"Saving processed data to: {OUTPUT_PATH}\")\n\nwith open(OUTPUT_PATH, 'wb') as f:\n    pickle.dump(processed_data, f)\n\nprint(\"\\n--- PREPROCESSING COMPLETE! ---\")\nprint(\"Your final dataset is ready. You can now proceed to the noise augmentation step.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:07:06.065241Z","iopub.execute_input":"2025-08-19T06:07:06.066024Z","iopub.status.idle":"2025-08-19T06:27:02.706691Z","shell.execute_reply.started":"2025-08-19T06:07:06.065992Z","shell.execute_reply":"2025-08-19T06:27:02.705088Z"}},"outputs":[{"name":"stdout","text":"Starting final processing with corrected file paths...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Meetings:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69428e1ef1e44d89a7003f530c2f51f9"}},"metadata":{}},{"name":"stdout","text":"\n✅ Successfully generated 12026 labeled audio clips.\nSaving processed data to: /kaggle/working/processed_ami_data.pkl\n\n--- PREPROCESSING COMPLETE! ---\nYour final dataset is ready. You can now proceed to the noise augmentation step.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# PHASE 2: Augment with Noise from the Correct Path\n\nimport numpy as np\nimport soundfile as sf\nfrom tqdm.notebook import tqdm\nimport random\nimport pickle\n\nprint(\"Starting Phase 2: Noise Augmentation with corrected path...\")\n\n# --- Configuration ---\n# --- THIS IS THE CORRECTED PATH ---\nNOISE_PATH = \"ring-ami-dataset-storage/MiniLibriMix/MiniLibriMix/train/mix_both\"\n\nSNR_LEVEL_DB = 10\nINPUT_PATH = \"/kaggle/working/processed_ami_data.pkl\"\nFINAL_OUTPUT_PATH = \"/kaggle/working/final_labeled_dataset.pkl\"\n\n# --- Load the Data from Phase 1 ---\nprint(f\"Loading data from {INPUT_PATH}...\")\nwith open(INPUT_PATH, 'rb') as f:\n    processed_data = pickle.load(f)\n\n# --- Load Noise Files ---\nprint(f\"Finding all noise files in: {NOISE_PATH}\")\ntry:\n    # We don't need a recursive search anymore as we have the full path\n    noise_files = fs.glob(f\"{NOISE_PATH}/*.wav\")\n    print(f\"Found {len(noise_files)} noise files.\")\nexcept Exception as e:\n    print(f\"❌ Could not find noise files. Error: {e}\")\n    noise_files = []\n\n# --- Augmentation Loop ---\nnoisy_samples = []\nif noise_files:\n    speech_clips = [data for data in processed_data if data['label'] == 'Speech / Monologue']\n\n    for speech_sample in tqdm(speech_clips, desc=\"Augmenting with Noise\"):\n        clean_audio = speech_sample['ihm_audio']\n        \n        random_noise_file = random.choice(noise_files)\n        with fs.open(random_noise_file, 'rb') as f:\n            noise_audio, sr_noise = sf.read(f)\n            \n        if len(noise_audio) < len(clean_audio):\n            repeats = int(np.ceil(len(clean_audio) / len(noise_audio)))\n            noise_audio = np.tile(noise_audio, repeats)\n        \n        noise_segment = noise_audio[:len(clean_audio)]\n        \n        speech_power = np.mean(clean_audio ** 2)\n        noise_power = np.mean(noise_segment ** 2)\n        if noise_power > 1e-6:\n            snr_factor = np.sqrt(speech_power / (10**(SNR_LEVEL_DB / 10) * noise_power))\n            noisy_audio = clean_audio + (noise_segment * snr_factor)\n        else:\n            noisy_audio = clean_audio\n        \n        noisy_samples.append({\n            \"meeting_id\": speech_sample['meeting_id'], \"label\": \"Noisy Environment\",\n            \"ihm_audio\": noisy_audio, \"sdm_audio\": noisy_audio,\n            \"sample_rate\": speech_sample['sample_rate'],\n        })\n\n    print(f\"\\n✅ Generated {len(noisy_samples)} 'Noisy Environment' samples.\")\n    processed_data.extend(noisy_samples)\n    print(f\"Total processed samples now: {len(processed_data)}\")\nelse:\n    print(\"Skipping noise augmentation as no noise files were found.\")\n\n# --- Save the Final, Combined Dataset ---\nprint(f\"\\nSaving final combined dataset with all 4 scenarios to: {FINAL_OUTPUT_PATH}\")\nwith open(FINAL_OUTPUT_PATH, 'wb') as f:\n    pickle.dump(processed_data, f)\n    \nprint(\"\\n--- DATASET FULLY COMPLETE! ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:50:51.308695Z","iopub.execute_input":"2025-08-19T06:50:51.311243Z","execution_failed":"2025-08-19T11:45:19.824Z"}},"outputs":[{"name":"stdout","text":"Starting Phase 2: Noise Augmentation with corrected path...\nLoading data from /kaggle/working/processed_ami_data.pkl...\nFinding all noise files in: ring-ami-dataset-storage/MiniLibriMix/MiniLibriMix/train/mix_both\nFound 800 noise files.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting with Noise:   0%|          | 0/5971 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db51951150540f38a7ca9183ef31b13"}},"metadata":{}},{"name":"stdout","text":"\n✅ Generated 5971 'Noisy Environment' samples.\nTotal processed samples now: 17997\n\nSaving final combined dataset with all 4 scenarios to: /kaggle/working/final_labeled_dataset.pkl\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}